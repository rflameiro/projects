{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "17cckAyOPGvw2iAdsINZfMzHMQt_9qNQ0",
      "authorship_tag": "ABX9TyPWF+1IWOpg9X7f5Yf/Sqjg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rflameiro/projects/blob/main/Bayes_Error_Rate_Estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import dataset\n",
        "\n",
        "Dataset: BBBP from MoleculeNet after removing compounds that failed standardization and conversion to Morgan fingerprints (1024 bits, radius=3)."
      ],
      "metadata": {
        "id": "wFxIxr2E5xCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "H8ag2QZq6HP8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dataset\n",
        "url = 'https://raw.githubusercontent.com/rflameiro/Python_e_Quiminformatica/main/datasets/BBBP_morganFP_1024_radius3.csv'\n",
        "df_fp = pd.read_csv(url, sep=\";\", index_col=False)"
      ],
      "metadata": {
        "id": "76Ey5b5Q4glp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_fp.iloc[:, :-1]\n",
        "y = df_fp.iloc[:, -1]\n",
        "\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "id": "BeoN9XWJEB2w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f23beb93-8eff-4e3c-bc91-ae2761c7829e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1934, 1024) (1934,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "s-gMtqY0EQjF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.to_numpy()\n",
        "X_test = X_test.to_numpy()\n",
        "y_train = y_train.to_numpy()\n",
        "y_test = y_test.to_numpy()"
      ],
      "metadata": {
        "id": "_AaRvFL0po70"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning to Benchmark\n",
        "\n",
        "Adapted from [GitHub](https://github.com/mrtnoshad/Bayes_Error_Estimator/blob/master/Example.ipynb)"
      ],
      "metadata": {
        "id": "g7Kkm3g_5QHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mrtnoshad/Bayes_Error_Estimator/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2soaayY5OKj",
        "outputId": "0ce4430d-b9f7-4b12-e483-686199bb3143"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Bayes_Error_Estimator' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important: I found two problems using this code on Colab:\n",
        "\n",
        "First, I had to delete the line `matplotlib.use('Agg')` on BER_estimator_KDtree.py to prevent `ImportError: Cannot load backend 'TkAgg' which requires the 'tk' interactive framework, as 'headless' is currently running`\n",
        "\n",
        "Also, on the same file, I had to to replace `asscalar` to `ndarray.item` as it is a deprecated function.\n",
        "\n",
        "Then I deleted the file on the Bayes_Error_Estimator folder and re-uploaded the fixed version. I already created a pull request and an issue on GitHub."
      ],
      "metadata": {
        "id": "UNo5qnBULXk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"/content/Bayes_Error_Estimator\")"
      ],
      "metadata": {
        "id": "_XKfLd8H5mUp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from BER_estimator_KDtree import ensemble_bg_estimator as BER"
      ],
      "metadata": {
        "id": "KOadLakvLR6K"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate Bayes Error Rate\n",
        "est_BER = BER(X_train, y_train)\n",
        "\n",
        "print(est_BER)"
      ],
      "metadata": {
        "id": "ghimFx965J-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c443a743-83a2-4475-b887-988323fb55b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.02134041599271922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This indicates that for the BBBP dataset the Bayes error rate is around 2%, which seems a bit low for a cheminformatics dataset. Let's use another method and compare."
      ],
      "metadata": {
        "id": "790ndtwYRqrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FeeBee\n",
        "\n",
        "These functions are used to estimate the lower and upper bounds of the BER for binary classifiers instead of just one value. They were adapted from https://github.com/DS3Lab/feebee/tree/main.\n",
        "\n",
        "`knn_eval_from_matrices_split()` was adapted from https://github.com/DS3Lab/feebee/blob/main/methods/knn.py#L145. It uses the k-NN method and the dataset needs to be split into train and test sets. I added the option to use Tanimoto distances, as this is widely used in cheminformatics.\n",
        "\n",
        "`ghp_eval_from_matrix()` was adapted from https://github.com/DS3Lab/feebee/blob/main/methods/ghp.py#L15. It uses the GHP method and only the train set is used."
      ],
      "metadata": {
        "id": "bFz5zb-TnBfx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncKTQT17y6kC",
        "outputId": "73546aa1-5cfa-4635-e3fc-1f88ff79e692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'feebee'...\n",
            "remote: Enumerating objects: 311, done.\u001b[K\n",
            "remote: Total 311 (delta 0), reused 0 (delta 0), pack-reused 311\u001b[K\n",
            "Receiving objects: 100% (311/311), 812.50 KiB | 8.46 MiB/s, done.\n",
            "Resolving deltas: 100% (165/165), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/DS3Lab/feebee"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"/content/feebee\")"
      ],
      "metadata": {
        "id": "POuaUoelzC7m"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import os.path as path\n",
        "import pandas as pd\n",
        "import random\n",
        "from scipy.sparse.csgraph import minimum_spanning_tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "from methods.utils import compute_distance_matrix_loo, knn_errorrate, knn_errorrate_loo, load_data, load_embedding_fn\n",
        "# use this instead of:\n",
        "# from methods.utils import *\n",
        "# because I reimplemented compute_distance_matrix\n",
        "\n",
        "import transformations.label_noise as label_noise"
      ],
      "metadata": {
        "id": "vOFKCTMKzj0m"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to calculate distances for kNN"
      ],
      "metadata": {
        "id": "C8Fu55DaJJsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import jaccard_score\n",
        "\n",
        "def compute_distance_matrix(x_train, x_test, measure=\"tanimoto\"):\n",
        "    \"\"\"Calculates the distance matrix between test and train.\n",
        "\n",
        "    Args:\n",
        "    x_train: Matrix (NxD) where each row represents a training sample\n",
        "    x_test: Matrix (MxD) where each row represents a test sample\n",
        "    measure: Distance measure (not necessarly metric) to use\n",
        "\n",
        "    Raises:\n",
        "    NotImplementedError: When the measure is not implemented\n",
        "\n",
        "    Returns:\n",
        "    Matrix (MxN) where element i,j is the distance between\n",
        "    x_test_i and x_train_j.\n",
        "\n",
        "    ------\n",
        "\n",
        "    Removed the option to use tensorflow\n",
        "    Added the option to use Tanimoto distances: measure=\"tanimoto\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Type check\n",
        "    if x_train.dtype != np.float32:\n",
        "        x_train = np.float32(x_train)\n",
        "    if x_test.dtype != np.float32:\n",
        "        x_test = np.float32(x_test)\n",
        "\n",
        "    # This method was taking too long, so I remade the code using matrix operations\n",
        "    # if measure == \"tanimoto\":\n",
        "        # M = x_test.shape[0]\n",
        "        # N = x_train.shape[0]\n",
        "\n",
        "        # # Initialize an empty MxN matrix for Jaccard distances\n",
        "        # x_xt = np.zeros((M, N))\n",
        "\n",
        "        # # Iterate through each pair of rows from x_test and x_train\n",
        "        # for i in range(M):\n",
        "        #     for j in range(N):\n",
        "        #         # Calculate the Jaccard distance between the i-th row of x_test and the j-th row of x_train\n",
        "        #         jaccard_distance = 1.0 - jaccard_score(x_test[i], x_train[j])\n",
        "\n",
        "        #         # Store the Jaccard distance in the matrix\n",
        "        #         x_xt[i, j] = jaccard_distance\n",
        "\n",
        "    if measure == \"tanimoto\":\n",
        "        # Assumes x_test and x_train are matrices in which each row is a binary vector.\n",
        "        intersection = np.dot(x_test, x_train.T)\n",
        "        union = np.sum(x_test, axis=1, keepdims=True) + np.sum(x_train, axis=1, keepdims=True).T - intersection\n",
        "        # Compute Jaccard distance matrix. np.where is used to account for division by zero errors\n",
        "        x_xt = np.where(union == 0, 0.0, 1.0 - (intersection / union))\n",
        "\n",
        "    elif measure == \"squared_l2\":\n",
        "        x_xt = np.matmul(x_test, np.transpose(x_train))\n",
        "\n",
        "        x_train_2 = np.sum(np.square(x_train), axis=1)\n",
        "        x_test_2 = np.sum(np.square(x_test), axis=1)\n",
        "\n",
        "        for i in range(np.shape(x_xt)[0]):\n",
        "            x_xt[i, :] = np.multiply(x_xt[i, :], -2)\n",
        "            x_xt[i, :] = np.add(x_xt[i, :], x_test_2[i])\n",
        "            x_xt[i, :] = np.add(x_xt[i, :], x_train_2)\n",
        "\n",
        "    elif measure == \"cosine\":\n",
        "        x_xt = np.matmul(x_test, np.transpose(x_train))\n",
        "\n",
        "        x_train_2 = np.linalg.norm(x_train, axis=1)\n",
        "        x_test_2 = np.linalg.norm(x_test, axis=1)\n",
        "\n",
        "        outer = np.outer(x_test_2, x_train_2)\n",
        "        x_xt = np.ones(np.shape(x_xt)) - np.divide(x_xt, outer)\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError(\"Method '{}' is not implemented\".format(measure))\n",
        "\n",
        "    return x_xt"
      ],
      "metadata": {
        "id": "4ptzv-0p0Hat"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BER estimator: kNN"
      ],
      "metadata": {
        "id": "9SCRa2tkJFOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# knn.py\n",
        "\n",
        "KEY_PATTERN = \"measure={0}, k={1}\"\n",
        "\n",
        "def knn_get_lowerbound(value, k, classes):\n",
        "\n",
        "    if value <= 1e-10:\n",
        "        return 0.0\n",
        "\n",
        "    if classes > 2 or k == 1:\n",
        "        return ((classes - 1.0)/float(classes)) * (1.0 - math.sqrt(max(0.0, 1 - ((float(classes) / (classes - 1.0)) * value))))\n",
        "\n",
        "    if k > 2:\n",
        "        return value / float(1 + (1.0/math.sqrt(k)))\n",
        "\n",
        "    return value / float(1 + math.sqrt(2.0/k))\n",
        "\n",
        "\n",
        "def knn_eval_from_matrices_split(train_features, test_features, train_labels,\n",
        "                                 test_labels, knn_measure='tanimoto',\n",
        "                                 knn_k=[1,3,5,7,9], knn_subtest=None, knn_subtrain=None):\n",
        "\n",
        "    total_classes = np.unique(np.concatenate((train_labels, test_labels))).size\n",
        "    vals = {}\n",
        "    ks = sorted(set(knn_k), reverse=True)\n",
        "\n",
        "    if knn_subtest is not None:\n",
        "        test_dividable = (test_features.shape[0] % knn_subtest) == 0\n",
        "        test_iterations = (test_features.shape[0] + knn_subtest - 1 ) // knn_subtest\n",
        "    else:\n",
        "        test_dividable = True\n",
        "        test_iterations = 1\n",
        "\n",
        "    for i in range(test_iterations):\n",
        "        if test_iterations == 1:\n",
        "            current_pos = 0\n",
        "            current_samples = test_features.shape[0]\n",
        "        else:\n",
        "            current_pos = i*knn_subtest\n",
        "            current_samples = min(test_features.shape[0],(i+1)*knn_subtest) - current_pos\n",
        "\n",
        "        # # if multiple train iterations\n",
        "        # if knn_subtrain is not None:\n",
        "        #     # select per split to nearest k (for max k) train point distances and indices\n",
        "        #     train_iterations = (train_features.shape[0] + knn_subtrain -1) // knn_subtrain\n",
        "        #     d = None\n",
        "        #     y_train = None\n",
        "        #     for j in range(train_iterations):\n",
        "        #         start_idx = j * knn_subtrain\n",
        "        #         end_idx = min(train_features.shape[0], (j+1) * knn_subtrain)\n",
        "\n",
        "        #         start = time.time()\n",
        "        #         d_j = compute_distance_matrix(train_features[start_idx:end_idx,:], test_features[current_pos:(current_pos + current_samples),:], knn_measure)\n",
        "        #         end = time.time()\n",
        "\n",
        "\n",
        "        #         if end_idx - start_idx < ks[0]:\n",
        "        #             # do not run argpartition\n",
        "        #             sub_d = d_j\n",
        "        #             indices = np.tile(range(start_idx, end_idx), (sub_d.shape[0],1))\n",
        "        #         else:\n",
        "        #             # run argpartition and build\n",
        "        #             indices = np.argpartition(d_j, ks[0] - 1, axis=1)\n",
        "        #             # update sub_d and y_train_new\n",
        "        #             num_rows = indices[:, :ks[0]].shape[0]\n",
        "        #             num_cols = indices[:, :ks[0]].shape[1]\n",
        "        #             rows = [x for x in range(num_rows) for _ in range(num_cols)]\n",
        "        #             cols = indices[:, :ks[0]].reshape(-1)\n",
        "        #             sub_d = d_j[rows, cols].reshape(num_rows, -1)\n",
        "\n",
        "        #             indices = indices[:, :ks[0]] + start_idx\n",
        "\n",
        "        #         y_train_new = indices\n",
        "        #         for k in range(y_train_new.shape[0]):\n",
        "        #             y_train_new[k, :] = train_labels[y_train_new[k, :]]\n",
        "\n",
        "        #         if d is None:\n",
        "        #             d = sub_d\n",
        "        #         else:\n",
        "        #             d = np.concatenate((d, sub_d), axis=1)\n",
        "\n",
        "        #         if y_train is None:\n",
        "        #             y_train = y_train_new\n",
        "        #         else:\n",
        "        #             y_train = np.concatenate((y_train, y_train_new), axis=1)\n",
        "\n",
        "        #     # run knn on that smaller matrix\n",
        "\n",
        "        #     start = time.time()\n",
        "        #     err = knn_errorrate(d,\n",
        "        #                         y_train,\n",
        "        #                         test_labels[current_pos:(current_pos + current_samples)],\n",
        "        #                         k=ks)\n",
        "        #     end = time.time()\n",
        "\n",
        "\n",
        "        # else:\n",
        "\n",
        "        start = time.time()\n",
        "        d = compute_distance_matrix(train_features, test_features[current_pos:(current_pos + current_samples),:], knn_measure)\n",
        "        end = time.time()\n",
        "\n",
        "        start = time.time()\n",
        "        err = knn_errorrate(d,\n",
        "                            train_labels,\n",
        "                            test_labels[current_pos:(current_pos + current_samples)],\n",
        "                            k=ks)\n",
        "        end = time.time()\n",
        "\n",
        "\n",
        "        if not test_dividable:\n",
        "            err = [e * (current_samples/test_features.shape[0]) for e in err]\n",
        "        else:\n",
        "            err = [e * (1.0/test_iterations) for e in err]\n",
        "\n",
        "        for idx, k in enumerate(ks):\n",
        "            if k not in vals:\n",
        "                vals[k] = err[idx]\n",
        "            else:\n",
        "                vals[k] += err[idx]\n",
        "\n",
        "    results = {}\n",
        "    for k, v in vals.items():\n",
        "        results[KEY_PATTERN.format(knn_measure, k)] = [v, knn_get_lowerbound(v, k, total_classes)]\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "XaiXvlWB0bPg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BER estimator: GHP"
      ],
      "metadata": {
        "id": "xadiocDrJCps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ghp.py\n",
        "\n",
        "def ghp_eval_from_matrix(train_features, train_labels, ghp_approx=None):\n",
        "\n",
        "    start = time.time()\n",
        "    d = compute_distance_matrix_loo(train_features, \"cosine\")\n",
        "    #d = np.sqrt(compute_distance_matrix_loo(train_features, \"squared_l2\"))\n",
        "\n",
        "    if ghp_approx:\n",
        "        indices = np.argpartition(d, ghp_approx, axis=1)[:,:ghp_approx]\n",
        "        for row_i in range(d.shape[0]):\n",
        "            mask = np.ones(d.shape[1],dtype=bool)\n",
        "            mask[indices[row_i,:]] = False\n",
        "            d[row_i, mask] = 0.0\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    start = time.time()\n",
        "    d = minimum_spanning_tree(d).tocoo()\n",
        "    end = time.time()\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    classes, classes_counts = np.unique(train_labels, return_counts=True)\n",
        "\n",
        "    num_train_samples = train_labels.size\n",
        "    num_classes = len(classes)\n",
        "\n",
        "    mapping = {}\n",
        "    idx = 0\n",
        "    for c in classes:\n",
        "        mapping[c] = idx\n",
        "        idx += 1\n",
        "\n",
        "    deltas = []\n",
        "    for i in range(num_classes-1):\n",
        "        deltas.append([0.0]*(num_classes-i-1))\n",
        "\n",
        "    # Calculate number of dichotomous edges\n",
        "    for i in range(num_train_samples - 1):\n",
        "        label_1 = mapping[train_labels[d.row[i]]]\n",
        "        label_2 = mapping[train_labels[d.col[i]]]\n",
        "        if label_1 == label_2:\n",
        "            continue\n",
        "        if label_1 > label_2:\n",
        "            tmp = label_1\n",
        "            label_1 = label_2\n",
        "            label_2 = tmp\n",
        "        deltas[label_1][label_2 - label_1 - 1] += 1\n",
        "\n",
        "    # Divide the number of dichotomous edges by 2 * num_train_samples to get estimator of deltas\n",
        "    deltas = [[item / (2.0 * num_train_samples) for item in sublist] for sublist in deltas]\n",
        "\n",
        "    # Sum up all the deltas\n",
        "    delta_sum = sum([sum(sublist) for sublist in deltas])\n",
        "\n",
        "    end = time.time()\n",
        "    # logging.log(logging.DEBUG, \"Estimators computed in {} seconds\".format(end - start))\n",
        "\n",
        "    upper = 2.0 * delta_sum\n",
        "\n",
        "    lower = ((num_classes - 1.0) / float(num_classes)) * (1.0 - math.sqrt(max(0.0, 1.0 - ((2.0 * num_classes)/(num_classes - 1.0) * delta_sum))))\n",
        "\n",
        "    return {\"default\": [upper, lower]}"
      ],
      "metadata": {
        "id": "jWkK1ndH44Go"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimate lower and upper bounds of BER on MoleculeNet BBBP dataset"
      ],
      "metadata": {
        "id": "xbeL6uvoI-LD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# kNN cosine\n",
        "df = knn_eval_from_matrices_split(X_train, X_test, y_train, y_test, knn_measure='cosine')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNgzjqM1IvMG",
        "outputId": "a26a1fc2-abce-4d49-fe48-49aa767a5e1e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'measure=cosine, k=9': [0.12144702842377261, 0.09108527131782947],\n",
              " 'measure=cosine, k=7': [0.12144702842377261, 0.08813509404821888],\n",
              " 'measure=cosine, k=5': [0.10852713178294573, 0.07499040371124602],\n",
              " 'measure=cosine, k=3': [0.11627906976744186, 0.07371797630413504],\n",
              " 'measure=cosine, k=1': [0.12661498708010335, 0.0679207173909534]}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# kNN Tanimoto\n",
        "df = knn_eval_from_matrices_split(X_train, X_test, y_train, y_test, knn_measure='tanimoto')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUo-wu7lyw2G",
        "outputId": "9a87e466-2483-4290-fc40-60fa2d62b551"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'measure=tanimoto, k=9': [0.11627906976744186, 0.0872093023255814],\n",
              " 'measure=tanimoto, k=7': [0.12403100775193798, 0.09001030881520225],\n",
              " 'measure=tanimoto, k=5': [0.10852713178294573, 0.07499040371124602],\n",
              " 'measure=tanimoto, k=3': [0.11627906976744186, 0.07371797630413504],\n",
              " 'measure=tanimoto, k=1': [0.13178294573643412, 0.07092130426717413]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GHP - uses cosine distance\n",
        "df = ghp_eval_from_matrix(X_train, y_train)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INXgdUVV4nZZ",
        "outputId": "f211b320-e733-427f-acfe-978a5338123b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /content/feebee/methods/utils.py:104: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'default': [0.16095669036845509, 0.08826992481023144]}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that the Bayes error rate for the BBBP dataset is between 0.07 and 0.12, which translates to an accuracy range of 88-93%. I believe this is a more reasonable estimate than the one from \"Learning to Benchmark\".\n",
        "\n",
        "Let's see what we can get with a Random Forest model:"
      ],
      "metadata": {
        "id": "W6ve9PCD3oZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# cross-validation on the training set\n",
        "cv_scores = cross_val_score(rf_classifier, X_train, y_train, cv=5)\n",
        "\n",
        "# fit, predict test set\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
        "print(\"Test Set Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGZATg38p8Hw",
        "outputId": "4300f59e-3864-46fb-b1f7-ccc7ce7c3b89"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.88064516 0.87741935 0.86731392 0.89967638 0.87055016]\n",
            "Mean CV Accuracy: 0.8791209938406931\n",
            "Test Set Accuracy: 0.875968992248062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got an accuracy estimate close to the upper bound of our BER estimate, so there might still be room for improvement on our model."
      ],
      "metadata": {
        "id": "Qj_DJFGL4lad"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eslAHG3bUcEM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}